{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlFPjfsEISm0jOK2Zae1wl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaiMiranda/reddit-sentiment-analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "id": "2SB9CoFqWqNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3d8868-7cf0-4f5c-bc43-5f5882ca75bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.7.14)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "7LiGNhivK3UK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281531cb-c005-41d7-d89b-ffe828714325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "import praw\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import logging\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-E2AGYrKIqZ",
        "outputId": "de780ca2-d6a3-43d6-f489-a9949fe6aafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reddit API Credentials\n",
        "from google.colab import userdata\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id = userdata.get('clientID'),\n",
        "    client_secret = userdata.get('clientSecret'),\n",
        "    user_agent = \"SentimentAnalyzer1.0\"\n",
        ")"
      ],
      "metadata": {
        "id": "dN6pLkESM-1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the subreddit you want to analyze\n",
        "subreddit_name = input(\"Enter a subreddit name: \")\n",
        "\n",
        "# Get the subreddit\n",
        "subreddit = reddit.subreddit(subreddit_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ogvPsu4MjKE",
        "outputId": "43cf34f1-af9b-48c5-c119-8b10c6ea48bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a subreddit name: biltrewards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the dates you want to analyze data for\n",
        "# Loop to get valid date input\n",
        "while True:\n",
        "    start_date_str = input(\"Enter start date (MM-DD-YYYY): \")\n",
        "    end_date_str = input(\"Enter end date (MM-DD-YYYY): \")\n",
        "\n",
        "    # Error checking for timestamps\n",
        "    try:\n",
        "        start_datetime = datetime.datetime.strptime(start_date_str, \"%m-%d-%Y\")\n",
        "        end_datetime = datetime.datetime.strptime(end_date_str, \"%m-%d-%Y\").replace(hour=23, minute=59, second=59)\n",
        "\n",
        "        if end_datetime < start_datetime:\n",
        "            print(\"End date must be after start date. Please try again.\")\n",
        "            continue # Ask for dates again if end date is before start date\n",
        "\n",
        "        start_timestamp = start_datetime.timestamp()\n",
        "        end_timestamp = end_datetime.timestamp()\n",
        "        break # Exit loop if dates are valid and in correct order\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid date format. Please use MM-DD-YYYY.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1562922-6924-4225-8752-4ab52c1fcedc",
        "id": "WHPAo7zKMBSs"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter start date (MM-DD-YYYY): 01-01-2025\n",
            "Enter end date (MM-DD-YYYY): 06-30-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vE4qLQrBWs6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c26152-9a9c-4b4d-df17-e6661d5db366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 100 posts so far...\n",
            "Fetched 200 posts so far...\n",
            "Fetched 300 posts so far...\n",
            "Fetched 400 posts so far...\n",
            "Fetched 500 posts so far...\n",
            "Fetched 600 posts so far...\n",
            "Fetched 700 posts so far...\n",
            "Fetched 800 posts so far...\n",
            "Fetched 900 posts so far...\n",
            "Fetched 1000 posts so far...\n",
            "Fetched 1100 posts so far...\n",
            "Fetched 1200 posts so far...\n",
            "Fetched 1300 posts so far...\n",
            "Fetched 1400 posts so far...\n",
            "Fetched 1500 posts so far...\n",
            "Fetched 1600 posts so far...\n",
            "Fetched 1700 posts so far...\n",
            "Fetched 1800 posts so far...\n",
            "Fetched 1900 posts so far...\n",
            "Fetched 2000 posts so far...\n",
            "Fetched 2100 posts so far...\n",
            "Fetched 2200 posts so far...\n",
            "Fetched 2300 posts so far...\n",
            "Fetched 2400 posts so far...\n",
            "Fetched 2500 posts so far...\n",
            "Fetched 2600 posts so far...\n",
            "Fetched 2700 posts so far...\n",
            "Fetched 2800 posts so far...\n",
            "Fetched 2900 posts so far...\n",
            "Fetched 3000 posts so far...\n",
            "Fetched 3100 posts so far...\n",
            "Fetched 3200 posts so far...\n",
            "Fetched 3300 posts so far...\n",
            "Fetched 3400 posts so far...\n",
            "Fetched 3500 posts so far...\n",
            "Fetched 3600 posts so far...\n",
            "Fetched 3700 posts so far...\n",
            "Fetched 3800 posts so far...\n",
            "Fetched 3900 posts so far...\n",
            "Fetched 4000 posts so far...\n",
            "Fetched 4100 posts so far...\n",
            "Fetched 4200 posts so far...\n",
            "Fetched 4300 posts so far...\n",
            "Fetched 4400 posts so far...\n",
            "Fetched 4500 posts so far...\n",
            "Fetched 4600 posts so far...\n",
            "Fetched 4700 posts so far...\n",
            "Fetched 4800 posts so far...\n",
            "Fetched 4900 posts so far...\n",
            "Fetched 5000 posts so far...\n",
            "Fetched 5100 posts so far...\n",
            "Fetched 5200 posts so far...\n",
            "Fetched 5300 posts so far...\n",
            "Fetched 5400 posts so far...\n",
            "Fetched 5500 posts so far...\n",
            "Fetched 5600 posts so far...\n",
            "Fetched 5700 posts so far...\n",
            "Fetched 5800 posts so far...\n",
            "Fetched 5900 posts so far...\n",
            "Fetched 6000 posts so far...\n",
            "Fetched 6100 posts so far...\n",
            "Fetched 6200 posts so far...\n",
            "Fetched 6300 posts so far...\n",
            "Fetched 6400 posts so far...\n",
            "Fetched 6500 posts so far...\n",
            "Fetched 6600 posts so far...\n",
            "Fetched 6700 posts so far...\n",
            "Could not fetch comments for post 1k5kp2h: received 429 HTTP response\n",
            "Fetched 6800 posts so far...\n",
            "Could not fetch comments for post 1k5cep6: received 429 HTTP response\n",
            "Could not fetch comments for post 1k5b6we: received 429 HTTP response\n",
            "Could not fetch comments for post 1k580k0: received 429 HTTP response\n",
            "Could not fetch comments for post 1k500ki: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4zk5z: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4z0bm: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4ysnb: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4ynmv: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4y5sv: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4xfvt: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4xf42: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4wkan: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4oy1j: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4oed1: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4mj7e: received 429 HTTP response\n",
            "Could not fetch comments for post 1k4k7ow: received 429 HTTP response\n",
            "\n",
            "Collected 698 posts and 6129 comments within the specified date range.\n"
          ]
        }
      ],
      "source": [
        "# Get rid of the PRAW warnings\n",
        "logging.disable(logging.CRITICAL)\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Begin fetching posts\n",
        "fetched_posts_count = 0\n",
        "for submission in subreddit.new(limit=None): # Fetching all posts (no limit)\n",
        "  created_utc_ts = submission.created_utc\n",
        "  if created_utc_ts < start_timestamp:\n",
        "    break\n",
        "\n",
        "  if start_timestamp <= created_utc_ts <= end_timestamp:\n",
        "    submission_data = {\n",
        "      'type': 'post',\n",
        "      'id': submission.id,\n",
        "      'title': submission.title,\n",
        "      'text': submission.selftext,\n",
        "      'score': submission.score,\n",
        "      'num_comments': submission.num_comments,\n",
        "      'created_utc': datetime.datetime.fromtimestamp(created_utc_ts),\n",
        "      'url': submission.url,\n",
        "      'author': submission.author.name if submission.author else '[deleted]',\n",
        "      'parent_id': None\n",
        "    }\n",
        "    data.append(submission_data)\n",
        "\n",
        "    fetched_posts_count += 1\n",
        "    # Print progress every 100 posts to stay updated\n",
        "    if fetched_posts_count % 100 == 0:\n",
        "      print(f\"Fetched {fetched_posts_count} posts so far...\")\n",
        "\n",
        "  # Fetch all comments for the post\n",
        "  try:\n",
        "    submission.comments.replace_more(limit=None) # Fetching all comments (no limit)\n",
        "    all_comments = submission.comments.list()\n",
        "  except Exception as e:\n",
        "    print(f\"Could not fetch comments for post {submission.id}: {e}\")\n",
        "    all_comments = []\n",
        "\n",
        "  for comment in all_comments:\n",
        "    # Make sure we have an actual comment and not a MoreComments object\n",
        "    if not isinstance(comment, praw.models.MoreComments):\n",
        "      comment_created_utc_ts = comment.created_utc\n",
        "      # Only include comments within the date range as well\n",
        "      if start_timestamp <= comment_created_utc_ts <= end_timestamp:\n",
        "        comment_data = {\n",
        "          'type': 'comment',\n",
        "          'id': comment.id,\n",
        "          'title': submission.title,\n",
        "          'text': comment.body,\n",
        "          'score': comment.score,\n",
        "          'num_comments': 0,\n",
        "          'created_utc': datetime.datetime.fromtimestamp(comment_created_utc_ts),\n",
        "          'url': f\"https://www.reddit.com{comment.permalink}\",\n",
        "          'author': comment.author.name if comment.author else '[deleted]',\n",
        "          'parent_id': comment.parent_id\n",
        "        }\n",
        "        data.append(comment_data)\n",
        "\n",
        "        fetched_posts_count += 1\n",
        "        # Print progress every 100 posts to stay updated\n",
        "        if fetched_posts_count % 100 == 0:\n",
        "          print(f\"Fetched {fetched_posts_count} posts so far...\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(f\"\\nCollected {len(df[df['type'] == 'post'])} posts and {len(df[df['type'] == 'comment'])} comments within the specified date range.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment scores for a given text\n",
        "def get_sentiment_scores(text):\n",
        "    if not isinstance(text, str):\n",
        "        # Handle non-string values (e.g., NaN, None) by returning neutral scores\n",
        "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
        "    return analyzer.polarity_scores(text)\n",
        "\n",
        "# Apply the function to your 'Content Text' column\n",
        "# This will create a new column 'sentiment_scores' containing dictionaries\n",
        "df['sentiment_scores'] = df['text'].apply(get_sentiment_scores)\n",
        "\n",
        "# Extract individual scores into separate columns for easier use\n",
        "df['Negative Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['neg'])\n",
        "df['Neutral Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['neu'])\n",
        "df['Positive Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['pos'])\n",
        "df['Compound Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['compound']) # This is the main aggregated score\n",
        "\n",
        "# Drop the intermediate 'sentiment_scores' column\n",
        "df = df.drop(columns=['sentiment_scores'])\n",
        "\n",
        "# Categorize the compound score\n",
        "def categorize_sentiment(compound_score):\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Create a sentiment category column\n",
        "df['Sentiment Category'] = df['Compound Sentiment Score'].apply(categorize_sentiment)\n",
        "\n",
        "print(\"Sentiment analysis complete! New sentiment columns added to DataFrame.\")\n",
        "print(df[['title', 'text', 'Compound Sentiment Score', 'Sentiment Category']].head())"
      ],
      "metadata": {
        "id": "X7faZ2CcX8tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933377a4-3860-4b90-e586-659221b9b2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! New sentiment columns added to DataFrame.\n",
            "                                               title  \\\n",
            "0       Apartment using BILT. Should I get the card?   \n",
            "1       Apartment using BILT. Should I get the card?   \n",
            "2       Apartment using BILT. Should I get the card?   \n",
            "3       Apartment using BILT. Should I get the card?   \n",
            "4  How long do you think Alaska will last as a Bi...   \n",
            "\n",
            "                                                text  \\\n",
            "0  Hi all, I'm about to move into my first apartm...   \n",
            "1  I have the BILT card and it's pretty much like...   \n",
            "2  I just booked business class tickets for mysel...   \n",
            "3  Important to note for OP's reference the  bill...   \n",
            "4  Since Alaska airlines acquired Hawaiian, you c...   \n",
            "\n",
            "   Compound Sentiment Score Sentiment Category  \n",
            "0                    0.4939           Positive  \n",
            "1                    0.9217           Positive  \n",
            "2                    0.0000            Neutral  \n",
            "3                    0.2542           Positive  \n",
            "4                    0.8573           Positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export dataframe as a CSV file\n",
        "\n",
        "# Handle potential missing text content\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# Define the exact columns we want to include in our CSV.\n",
        "selected_columns_original_names = [\n",
        "    'id', 'type', 'title', 'text', 'score',\n",
        "    'num_comments', 'created_utc', 'url', 'author', 'parent_id',\n",
        "    'Negative Sentiment Score', 'Neutral Sentiment Score', 'Positive Sentiment Score',\n",
        "    'Compound Sentiment Score', 'Sentiment Category'\n",
        "]\n",
        "\n",
        "# Create the DataFrame that will be exported to CSV, containing only selected columns\n",
        "# Use .copy() is to avoid SettingWithCopyWarning later\n",
        "try:\n",
        "    export_df = df[selected_columns_original_names].copy()\n",
        "except KeyError as e:\n",
        "    print(f\"Error: One of the selected columns does not exist in your DataFrame: {e}\")\n",
        "    print(\"Please check `df.columns.tolist()` above and adjust `selected_columns_original_names` list.\")\n",
        "    raise e\n",
        "\n",
        "# Rename columns for clarity\n",
        "rename_map = {\n",
        "    'created_utc': 'Date Created UTC',\n",
        "    'score': 'Content Score (Upvotes/Downvotes)',\n",
        "    'num_comments': 'Number of Comments (for Posts)',\n",
        "    'author': 'Author',\n",
        "    'title': 'Post Title',\n",
        "    'text': 'Content Text',\n",
        "    'type': 'Content Type',\n",
        "    'url': 'Content URL',\n",
        "    'parent_id': 'Parent ID',\n",
        "    'Month Name': 'Month'\n",
        "}\n",
        "\n",
        "# Apply renaming\n",
        "export_df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Export to CSV\n",
        "output_filename = f\"{subreddit_name}_reddit_sentiment_data.csv\"\n",
        "\n",
        "# index=False: Prevents Pandas from writing the DataFrame index as a column in the CSV\n",
        "# encoding='utf-8-sig': improved compatibility\n",
        "# QUOTE_NONNUMERIC adds quotes to fields that not not purely numeric which helps prevent fields from being misinterpreted due to commas and other things\n",
        "export_df.to_csv(output_filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "print(f\"\\nCSV export complete!\")\n",
        "print(f\"\\nYour data is ready for analysis at: {output_filename}\")\n",
        "print(\"\\nNext steps: Open Your BI Tool of Choice and connect to this CSV file.\")"
      ],
      "metadata": {
        "id": "cImjd_X0Zdh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a09fcf-f973-4451-83be-429401076b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CSV export complete!\n",
            "\n",
            "Your data is ready for analysis at: biltrewards_reddit_sentiment_data.csv\n",
            "\n",
            "Next steps: Open Your BI Tool of Choice and connect to this CSV file.\n"
          ]
        }
      ]
    }
  ]
}