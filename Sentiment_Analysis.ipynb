{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWaTOFcpWugLaBDE0JgYno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaiMiranda/reddit-sentiment-analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import logging\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "l-E2AGYrKIqZ",
        "outputId": "dd8967fb-9847-4af0-c7c8-a0bda009bd68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reddit API Credentials\n",
        "from google.colab import userdata\n",
        "\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id = userdata.get('clientID'),\n",
        "    client_secret = userdata.get('clientSecret'),\n",
        "    user_agent = \"SentimentAnalyzerColab1.0\"\n",
        ")"
      ],
      "metadata": {
        "id": "dN6pLkESM-1-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the subreddit you want to analyze\n",
        "subreddit_name = input(\"Enter a subreddit name: \")\n",
        "\n",
        "# Get the subreddit\n",
        "subreddit = await reddit.subreddit(subreddit_name)"
      ],
      "metadata": {
        "id": "8ogvPsu4MjKE",
        "outputId": "e77918d1-0ae7-4b13-d905-c30f979c1d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a subreddit name: biltrewards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the dates you want to analyze data for\n",
        "# Loop to get valid date input\n",
        "while True:\n",
        "    start_date_str = input(\"Enter start date (MM-DD-YYYY): \")\n",
        "    end_date_str = input(\"Enter end date (MM-DD-YYYY): \")\n",
        "\n",
        "    # Error checking for timestamps\n",
        "    try:\n",
        "        start_datetime = datetime.datetime.strptime(start_date_str, \"%m-%d-%Y\")\n",
        "        end_datetime = datetime.datetime.strptime(end_date_str, \"%m-%d-%Y\").replace(hour=23, minute=59, second=59)\n",
        "\n",
        "        if end_datetime < start_datetime:\n",
        "            print(\"End date must be after start date. Please try again.\")\n",
        "            continue # Ask for dates again if end date is before start date\n",
        "\n",
        "        start_timestamp = start_datetime.timestamp()\n",
        "        end_timestamp = end_datetime.timestamp()\n",
        "        break # Exit loop if dates are valid and in correct order\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid date format. Please use MM-DD-YYYY.\")"
      ],
      "metadata": {
        "id": "WHPAo7zKMBSs",
        "outputId": "36937e32-5f95-46d1-fd59-c0bddcda1cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter start date (MM-DD-YYYY): 07-20-2025\n",
            "Enter end date (MM-DD-YYYY): 07-30-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": true,
        "id": "vE4qLQrBWs6I",
        "outputId": "6d85d519-8caa-4962-ab95-48dd8a394ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected 89 posts and 791 comments within the specified date range.\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Begin fetching posts\n",
        "async for submission in subreddit.new(limit=None): # Fetching all posts (no limit)\n",
        "\n",
        "  try:\n",
        "    await submission.load()\n",
        "  except asyncprawcore.exceptions.NotFound:\n",
        "    print(f\"Submission {submission.id} not found (likely deleted). Skipping.\")\n",
        "    continue # Skip to the next submission\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading submission {submission.id}: {e}. Skipping this post.\")\n",
        "    continue # Skip to the next submission\n",
        "\n",
        "  created_utc_ts = submission.created_utc\n",
        "  if created_utc_ts < start_timestamp:\n",
        "    break\n",
        "\n",
        "  if start_timestamp <= created_utc_ts <= end_timestamp:\n",
        "    submission_data = {\n",
        "      'type': 'post',\n",
        "      'id': submission.id,\n",
        "      'title': submission.title,\n",
        "      'text': submission.selftext,\n",
        "      'score': submission.score,\n",
        "      'num_comments': submission.num_comments,\n",
        "      'created_utc': datetime.datetime.fromtimestamp(created_utc_ts),\n",
        "      'url': submission.url,\n",
        "      'author': submission.author.name if submission.author else '[deleted]',\n",
        "      'parent_id': None\n",
        "    }\n",
        "    data.append(submission_data)\n",
        "\n",
        "  # Fetch all comments for the post\n",
        "  try:\n",
        "    await submission.comments.replace_more(limit=None) # Fetching all comments (no limit)\n",
        "    all_comments = await submission.comments.list()\n",
        "  except Exception as e:\n",
        "    print(f\"Could not fetch comments for post {submission.id}: {e}\")\n",
        "    all_comments = []\n",
        "\n",
        "  for comment in all_comments:\n",
        "    # Make sure we have an actual comment and not a MoreComments object\n",
        "    if not isinstance(comment, asyncpraw.models.MoreComments):\n",
        "      comment_created_utc_ts = comment.created_utc\n",
        "      # Only include comments within the date range as well\n",
        "      if start_timestamp <= comment_created_utc_ts <= end_timestamp:\n",
        "        comment_data = {\n",
        "          'type': 'comment',\n",
        "          'id': comment.id,\n",
        "          'title': submission.title,\n",
        "          'text': comment.body,\n",
        "          'score': comment.score,\n",
        "          'num_comments': 0,\n",
        "          'created_utc': datetime.datetime.fromtimestamp(comment_created_utc_ts),\n",
        "          'url': f\"https://www.reddit.com{comment.permalink}\",\n",
        "          'author': comment.author.name if comment.author else '[deleted]',\n",
        "          'parent_id': comment.parent_id\n",
        "        }\n",
        "        data.append(comment_data)\n",
        "\n",
        "await reddit.close()\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(f\"\\nCollected {len(df[df['type'] == 'post'])} posts and {len(df[df['type'] == 'comment'])} comments within the specified date range.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment scores for a given text\n",
        "def get_sentiment_scores(text):\n",
        "    if not isinstance(text, str):\n",
        "        # Handle non-string values (e.g., NaN, None) by returning neutral scores\n",
        "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
        "    return analyzer.polarity_scores(text)\n",
        "\n",
        "# Apply the function to your 'Content Text' column\n",
        "# This will create a new column 'sentiment_scores' containing dictionaries\n",
        "df['sentiment_scores'] = df['text'].apply(get_sentiment_scores)\n",
        "\n",
        "# Extract individual scores into separate columns for easier use\n",
        "df['Negative Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['neg'])\n",
        "df['Neutral Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['neu'])\n",
        "df['Positive Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['pos'])\n",
        "df['Compound Sentiment Score'] = df['sentiment_scores'].apply(lambda x: x['compound']) # This is the main aggregated score\n",
        "\n",
        "# Drop the intermediate 'sentiment_scores' column\n",
        "df = df.drop(columns=['sentiment_scores'])\n",
        "\n",
        "# Categorize the compound score\n",
        "def categorize_sentiment(compound_score):\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Create a sentiment category column\n",
        "df['Sentiment Category'] = df['Compound Sentiment Score'].apply(categorize_sentiment)\n",
        "\n",
        "print(\"Sentiment analysis complete! New sentiment columns added to DataFrame.\")\n",
        "print(df[['title', 'text', 'Compound Sentiment Score', 'Sentiment Category']].head())"
      ],
      "metadata": {
        "id": "X7faZ2CcX8tv",
        "outputId": "304015cc-dfc8-4703-ce14-8c4a8df16ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! New sentiment columns added to DataFrame.\n",
            "                                               title  \\\n",
            "0                               Partial Rent Payment   \n",
            "1   New to BILT and confused (currently also moving)   \n",
            "2              how to maximize bilt for rent rewards   \n",
            "3  Bilt Travel Portal Down for Bookings prior to ...   \n",
            "4                              New Bilt Cards coming   \n",
            "\n",
            "                                                text  \\\n",
            "0  Hello,\\n\\nI've come to an agreement (documente...   \n",
            "1  Hi, I've been interested in BILT for a while n...   \n",
            "2  hi,\\n\\ni live in a co-op and the management co...   \n",
            "3  Not sure why this portal is down...but can't a...   \n",
            "4  I got this in my newsfeed today about the new ...   \n",
            "\n",
            "   Compound Sentiment Score Sentiment Category  \n",
            "0                   -0.5650           Negative  \n",
            "1                    0.8689           Positive  \n",
            "2                    0.7971           Positive  \n",
            "3                   -0.5183           Negative  \n",
            "4                    0.0240            Neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export dataframe as a CSV file\n",
        "\n",
        "# Handle potential missing text content\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# Define the exact columns we want to include in our CSV.\n",
        "selected_columns_original_names = [\n",
        "    'id', 'type', 'title', 'text', 'score',\n",
        "    'num_comments', 'created_utc', 'url', 'author', 'parent_id',\n",
        "    'Negative Sentiment Score', 'Neutral Sentiment Score', 'Positive Sentiment Score',\n",
        "    'Compound Sentiment Score', 'Sentiment Category'\n",
        "]\n",
        "\n",
        "# Create the DataFrame that will be exported to CSV, containing only selected columns\n",
        "# Use .copy() is to avoid SettingWithCopyWarning later\n",
        "try:\n",
        "    export_df = df[selected_columns_original_names].copy()\n",
        "except KeyError as e:\n",
        "    print(f\"Error: One of the selected columns does not exist in your DataFrame: {e}\")\n",
        "    print(\"Please check `df.columns.tolist()` above and adjust `selected_columns_original_names` list.\")\n",
        "    raise e\n",
        "\n",
        "# Rename columns for clarity\n",
        "rename_map = {\n",
        "    'created_utc': 'Date Created UTC',\n",
        "    'score': 'Content Score (Upvotes/Downvotes)',\n",
        "    'num_comments': 'Number of Comments (for Posts)',\n",
        "    'author': 'Author',\n",
        "    'title': 'Post Title',\n",
        "    'text': 'Content Text',\n",
        "    'type': 'Content Type',\n",
        "    'url': 'Content URL',\n",
        "    'parent_id': 'Parent ID',\n",
        "    'Month Name': 'Month'\n",
        "}\n",
        "\n",
        "# Apply renaming\n",
        "export_df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "# Export to CSV\n",
        "output_filename = f\"{subreddit_name}_reddit_sentiment_data.csv\"\n",
        "\n",
        "# index=False: Prevents Pandas from writing the DataFrame index as a column in the CSV\n",
        "# encoding='utf-8-sig': improved compatibility\n",
        "# QUOTE_NONNUMERIC adds quotes to fields that not not purely numeric which helps prevent fields from being misinterpreted due to commas and other things\n",
        "export_df.to_csv(output_filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "print(f\"\\nCSV export complete!\")\n",
        "print(f\"\\nYour data is ready for analysis at: {output_filename}\")\n",
        "print(\"\\nNext steps: Open Your BI Tool of Choice and connect to this CSV file.\")"
      ],
      "metadata": {
        "id": "cImjd_X0Zdh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}